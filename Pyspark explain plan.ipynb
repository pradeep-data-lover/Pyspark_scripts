{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bfe031e-b13a-47bb-a5db-3819c9e5d0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>employee_id</th><th>name</th><th>doj</th><th>dept_id</th><th>gender</th><th>salary</th></tr></thead><tbody><tr><td>10</td><td>Raj</td><td>1991</td><td>100</td><td>M</td><td>2000</td></tr><tr><td>20</td><td>Rahul</td><td>2002</td><td>200</td><td>M</td><td>8000</td></tr><tr><td>30</td><td>Raghav</td><td>2010</td><td>100</td><td></td><td>6000</td></tr><tr><td>40</td><td>Raja</td><td>2004</td><td>100</td><td>F</td><td>7000</td></tr><tr><td>50</td><td>Rama</td><td>2008</td><td>400</td><td>F</td><td>1000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         10,
         "Raj",
         "1991",
         "100",
         "M",
         2000
        ],
        [
         20,
         "Rahul",
         "2002",
         "200",
         "M",
         8000
        ],
        [
         30,
         "Raghav",
         "2010",
         "100",
         "",
         6000
        ],
        [
         40,
         "Raja",
         "2004",
         "100",
         "F",
         7000
        ],
        [
         50,
         "Rama",
         "2008",
         "400",
         "F",
         1000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "employee_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doj",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "gender",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "employee_schema=[\"employee_id\",\"name\",\"doj\",\"dept_id\",\"gender\",\"salary\"]\n",
    "employee_data=[(10,\"Raj\",\"1991\",\"100\",\"M\",2000),\n",
    "               (20,\"Rahul\",\"2002\",\"200\",\"M\",8000),\n",
    "               (30,\"Raghav\",\"2010\",\"100\",\"\",6000),\n",
    "               (40,\"Raja\",\"2004\",\"100\",\"F\",7000),\n",
    "               (50,\"Rama\",\"2008\",\"400\",\"F\",1000)]\n",
    "employdf=spark.createDataFrame(data=employee_data,schema=employee_schema)\n",
    "display(employdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d956209-efbb-4c09-9862-feb654248d35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept_name</th><th>dept_id</th></tr></thead><tbody><tr><td>HR</td><td>100</td></tr><tr><td>supply</td><td>200</td></tr><tr><td>sales</td><td>300</td></tr><tr><td>stock</td><td>400</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "HR",
         100
        ],
        [
         "supply",
         200
        ],
        [
         "sales",
         300
        ],
        [
         "stock",
         400
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dept_id",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "depart_data=[(\"HR\",100),\n",
    "             (\"supply\",200),\n",
    "             (\"sales\",300),\n",
    "             (\"stock\",400)]\n",
    "depart_schema=[\"dept_name\",\"dept_id\"]\n",
    "departdf=spark.createDataFrame(data=depart_data,schema=depart_schema)\n",
    "display(departdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55d324f8-ccd5-4e1a-9745-f48c6b2b7fd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>dept_name</th><th>sum(salary)</th></tr></thead><tbody><tr><td>stock</td><td>1000</td></tr><tr><td>HR</td><td>15000</td></tr><tr><td>supply</td><td>8000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "stock",
         1000
        ],
        [
         "HR",
         15000
        ],
        [
         "supply",
         8000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "dept_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sum(salary)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfjoin=employdf.join(departdf,employdf.dept_id==departdf.dept_id,\"inner\")\\\n",
    "    .withColumn('bonus',col('salary')*0.01)\\\n",
    "        .groupBy(\"dept_name\").sum('salary').alias(\"total\")\n",
    "display(dfjoin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdc1fb96-26e7-4fce-8668-efdd7e100d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=true\n+- == Final Plan ==\n   ResultQueryStage 3, Statistics(sizeInBytes=108.0 B, rowCount=3, ColumnStat: N/A, isRuntime=true)\n   +- *(4) HashAggregate(keys=[dept_name#27], functions=[finalmerge_sum(merge sum#292L) AS sum(salary#7L)#285L])\n      +- AQEShuffleRead coalesced\n         +- ShuffleQueryStage 2, Statistics(sizeInBytes=96.0 B, rowCount=3, ColumnStat: N/A, isRuntime=true)\n            +- Exchange hashpartitioning(dept_name#27, 200), ENSURE_REQUIREMENTS, [plan_id=560]\n               +- *(3) HashAggregate(keys=[dept_name#27], functions=[partial_sum(salary#7L) AS sum#292L])\n                  +- *(3) Project [salary#7L, dept_name#27]\n                     +- *(3) SortMergeJoin [cast(dept_id#5 as bigint)], [dept_id#28L], Inner\n                        :- Sort [cast(dept_id#5 as bigint) ASC NULLS FIRST], false, 0\n                        :  +- AQEShuffleRead coalesced\n                        :     +- ShuffleQueryStage 0, Statistics(sizeInBytes=160.0 B, rowCount=5, ColumnStat: N/A, isRuntime=true)\n                        :        +- Exchange hashpartitioning(cast(dept_id#5 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=444]\n                        :           +- *(1) Project [dept_id#5, salary#7L]\n                        :              +- *(1) Filter isnotnull(dept_id#5)\n                        :                 +- *(1) Scan ExistingRDD[employee_id#2L,name#3,doj#4,dept_id#5,gender#6,salary#7L]\n                        +- Sort [dept_id#28L ASC NULLS FIRST], false, 0\n                           +- AQEShuffleRead coalesced\n                              +- ShuffleQueryStage 1, Statistics(sizeInBytes=128.0 B, rowCount=4, ColumnStat: N/A, isRuntime=true)\n                                 +- Exchange hashpartitioning(dept_id#28L, 200), ENSURE_REQUIREMENTS, [plan_id=449]\n                                    +- *(2) Filter isnotnull(dept_id#28L)\n                                       +- *(2) Scan ExistingRDD[dept_name#27,dept_id#28L]\n\n\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e91b740-c7a8-4e24-a285-5822c50e56e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\nSubqueryAlias total\n+- Aggregate [dept_name#27], [dept_name#27, sum(salary#7L) AS sum(salary)#379L]\n   +- Project [employee_id#2L, name#3, doj#4, dept_id#5, gender#6, salary#7L, dept_name#27, dept_id#28L, (cast(salary#7L as double) * 0.01) AS bonus#359]\n      +- Join Inner, (cast(dept_id#5 as bigint) = dept_id#28L)\n         :- LogicalRDD [employee_id#2L, name#3, doj#4, dept_id#5, gender#6, salary#7L], false\n         +- LogicalRDD [dept_name#27, dept_id#28L], false\n\n== Analyzed Logical Plan ==\ndept_name: string, sum(salary): bigint\nSubqueryAlias total\n+- Aggregate [dept_name#27], [dept_name#27, sum(salary#7L) AS sum(salary)#379L]\n   +- Project [employee_id#2L, name#3, doj#4, dept_id#5, gender#6, salary#7L, dept_name#27, dept_id#28L, (cast(salary#7L as double) * 0.01) AS bonus#359]\n      +- Join Inner, (cast(dept_id#5 as bigint) = dept_id#28L)\n         :- LogicalRDD [employee_id#2L, name#3, doj#4, dept_id#5, gender#6, salary#7L], false\n         +- LogicalRDD [dept_name#27, dept_id#28L], false\n\n== Optimized Logical Plan ==\nAggregate [dept_name#27], [dept_name#27, sum(salary#7L) AS sum(salary)#379L]\n+- Project [salary#7L, dept_name#27]\n   +- Join Inner, (cast(dept_id#5 as bigint) = dept_id#28L)\n      :- Project [dept_id#5, salary#7L]\n      :  +- Filter isnotnull(dept_id#5)\n      :     +- LogicalRDD [employee_id#2L, name#3, doj#4, dept_id#5, gender#6, salary#7L], false\n      +- Filter isnotnull(dept_id#28L)\n         +- LogicalRDD [dept_name#27, dept_id#28L], false\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   HashAggregate(keys=[dept_name#27], functions=[finalmerge_sum(merge sum#385L) AS sum(salary#7L)#378L], output=[dept_name#27, sum(salary)#379L])\n   +- Exchange hashpartitioning(dept_name#27, 200), ENSURE_REQUIREMENTS, [plan_id=905]\n      +- HashAggregate(keys=[dept_name#27], functions=[partial_sum(salary#7L) AS sum#385L], output=[dept_name#27, sum#385L])\n         +- Project [salary#7L, dept_name#27]\n            +- SortMergeJoin [cast(dept_id#5 as bigint)], [dept_id#28L], Inner\n               :- Sort [cast(dept_id#5 as bigint) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(cast(dept_id#5 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=897]\n               :     +- Project [dept_id#5, salary#7L]\n               :        +- Filter isnotnull(dept_id#5)\n               :           +- Scan ExistingRDD[employee_id#2L,name#3,doj#4,dept_id#5,gender#6,salary#7L]\n               +- Sort [dept_id#28L ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(dept_id#28L, 200), ENSURE_REQUIREMENTS, [plan_id=898]\n                     +- Filter isnotnull(dept_id#28L)\n                        +- Scan ExistingRDD[dept_name#27,dept_id#28L]\n\n"
     ]
    }
   ],
   "source": [
    "dfjoin.explain(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa789ec-b2f6-4d80-a278-debe2e2239e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Optimized Logical Plan ==\nAggregate [dept_name#27], [dept_name#27, sum(salary#7L) AS sum(salary)#379L], Statistics(sizeInBytes=8.18E+35 B, ColumnStat: N/A)\n+- Project [salary#7L, dept_name#27], Statistics(sizeInBytes=8.18E+35 B, ColumnStat: N/A)\n   +- Join Inner, (cast(dept_id#5 as bigint) = dept_id#28L), Statistics(sizeInBytes=1.45E+36 B, ColumnStat: N/A)\n      :- Project [dept_id#5, salary#7L], Statistics(sizeInBytes=2.8 EiB, ColumnStat: N/A)\n      :  +- Filter isnotnull(dept_id#5), Statistics(sizeInBytes=8.0 EiB, ColumnStat: N/A)\n      :     +- LogicalRDD [employee_id#2L, name#3, doj#4, dept_id#5, gender#6, salary#7L], false, Statistics(sizeInBytes=8.0 EiB, ColumnStat: N/A)\n      +- Filter isnotnull(dept_id#28L), Statistics(sizeInBytes=8.0 EiB, ColumnStat: N/A)\n         +- LogicalRDD [dept_name#27, dept_id#28L], false, Statistics(sizeInBytes=8.0 EiB, ColumnStat: N/A)\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   HashAggregate(keys=[dept_name#27], functions=[finalmerge_sum(merge sum#385L) AS sum(salary#7L)#378L], output=[dept_name#27, sum(salary)#379L])\n   +- Exchange hashpartitioning(dept_name#27, 200), ENSURE_REQUIREMENTS, [plan_id=905]\n      +- HashAggregate(keys=[dept_name#27], functions=[partial_sum(salary#7L) AS sum#385L], output=[dept_name#27, sum#385L])\n         +- Project [salary#7L, dept_name#27]\n            +- SortMergeJoin [cast(dept_id#5 as bigint)], [dept_id#28L], Inner\n               :- Sort [cast(dept_id#5 as bigint) ASC NULLS FIRST], false, 0\n               :  +- Exchange hashpartitioning(cast(dept_id#5 as bigint), 200), ENSURE_REQUIREMENTS, [plan_id=897]\n               :     +- Project [dept_id#5, salary#7L]\n               :        +- Filter isnotnull(dept_id#5)\n               :           +- Scan ExistingRDD[employee_id#2L,name#3,doj#4,dept_id#5,gender#6,salary#7L]\n               +- Sort [dept_id#28L ASC NULLS FIRST], false, 0\n                  +- Exchange hashpartitioning(dept_id#28L, 200), ENSURE_REQUIREMENTS, [plan_id=898]\n                     +- Filter isnotnull(dept_id#28L)\n                        +- Scan ExistingRDD[dept_name#27,dept_id#28L]\n\n\n"
     ]
    }
   ],
   "source": [
    "#dfjoin.explain(mode=\"simple\")\n",
    "#dfjoin.explain(mode=\"extended\")\n",
    "#dfjoin.explain(mode=\"formatted\")\n",
    "dfjoin.explain(mode=\"cost\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark explain plan",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
