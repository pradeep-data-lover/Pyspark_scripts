{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0c2121-0a29-4ce3-9aa4-65d2d6e18aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType,IntegerType,StringType,TimestampType,DecimalType,DoubleType,ShortType,BooleanType,ByteType,LongType\n",
    "def sparkdatatypeeq(dt):\n",
    "    if \"char\" in dt:\n",
    "        otpt_dt=StringType()\n",
    "    elif \"varchar\" == dt:\n",
    "        otpt_dt=StringType()\n",
    "    elif \"nvarchar\" == dt:\n",
    "        otpt_dt=StringType\n",
    "    elif \"date\" == dt:\n",
    "        otpt_dt=DateType()\n",
    "    elif \"datetime\" == dt:\n",
    "        otpt_dt=DateType()\n",
    "    elif \"datetime2\" == dt:\n",
    "        otpt_dt=DateType()\n",
    "    elif \"bit\" == dt:\n",
    "        otpt_dt=BooleanType()\n",
    "    elif \"int\" == dt:\n",
    "        otpt_dt=IntegerType()\n",
    "    elif \"tinyint\" == dt:\n",
    "        otpt_dt=IntegerType()\n",
    "    elif \"smallint\" == dt:\n",
    "        otpt_dt=IntegerType\n",
    "    elif \"decimal\" == dt:\n",
    "        otpt_dt=DecimalType(38,10)\n",
    "    elif \"numeric\" == dt:\n",
    "        otpt_dt=DecimalType(38,10)\n",
    "    elif \"float\" == dt:\n",
    "        otpt_dt=DoubleType()\n",
    "    else:\n",
    "        otpt_dt=StringType()\n",
    "    return otpt_dt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b440bb45-9fa7-499b-8d59-6ef3ea9a7a30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def castDataTypestoTarget(DF,table_name):\n",
    "    dbtable=table_name.split('.')\n",
    "    schema_name=dbtable[0]\n",
    "    table_name=dbtable[1]\n",
    "    qry=\"(select COLUMN_NAME,DATA_TYPE from information_schema.columns where table_name='{}' and table_schema='{}')schema_alias\".format(table_name,schema_name)\n",
    "    info_schema=spark.read.jdbc(url=jdbcurl,table=qry)\n",
    "    src_cols=DF.columns\n",
    "    for r in info_schema.collect():\n",
    "        if r.COLUMN_NAME in src_cols:\n",
    "            DF=Df.withColumn(r.COLUMN_NAME,col(r.COLUMN_NAME).cast(sparkdatatypeeq(r.DATE_TYPE)))\n",
    "        else:\n",
    "            DF=Df.withColumn(r.COLUMN_NAME,lit(None).cast(sparkdatatypeeq(r.DATE_TYPE)))\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08886e5f-67a3-44f0-aa7e-0ea61b918630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdbcHostname=\"ss-rajde.database.windows.net\"\n",
    "jdbcport=1433\n",
    "jdbcDatabase=\"sdb\"\n",
    "jdbcUsername=\"\"\n",
    "jdbcPassword=\"\"\n",
    "jdbcDriver=\"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "jdbcurl=f\"jdbc:sqlserver://{jdbcHostname}:{jdbcport};databaseName={jdbcDatabase};user={jdbcUsername};password={jdbcPassword}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd268ee-fc33-4148-8004-a636f2faa92e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "productDF=spark.read.format(\"csv\").option(\"header\",True).option(\"inferschema\",True),load(\"/FileStore/tables/adw-works\")\n",
    "prodDF=productDF.select([col(c).cast(\"string\") for  c in productDF.columns])\n",
    "prodDF.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e3019aa-ffad-4aa9-b2a1-41f29fb6982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prodDF.write.format('jdbc').option(url,jdbcurl).mode('overwrite').option(\"dbtable\",\"sales.Product\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5648a26b-4b69-4c6d-a1dd-68b04a111ebc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=castDataTypestoTarget(prodDF,\"sales.Product\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data type issue when writing in synapse",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
