{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a6346c-febe-4d74-89bc-ea9087c284c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5]\n1\n"
     ]
    }
   ],
   "source": [
    "rdd1=spark.sparkContext.parallelize(range(0,6),1)\n",
    "print(rdd1.collect())\n",
    "print(rdd1.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "449a9757-7b21-4f58-a617-cb0b652ab2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallelize : 4\nTextFile : 10\n"
     ]
    }
   ],
   "source": [
    "# Use parallelize with 6 partitions\n",
    "rdd2 = spark.sparkContext.parallelize(range(0,25), 4)\n",
    "print(\"parallelize : \"+str(rdd2.getNumPartitions()))\n",
    "\n",
    "rddFromFile = spark.sparkContext.textFile(\"/FileStore/tables/test.txt\",10)\n",
    "print(\"TextFile : \"+str(rddFromFile.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64f812e0-8a8b-4e9e-9b60-246c2b333514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\nFrom local[1] : 1\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.defaultParallelism)\n",
    "print(\"From local[1] : \"+str(rdd1.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8decc37f-23a7-4193-b3fa-03cab345bdc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd1.saveAsTextFile(\"/FileStore/tables/partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b90a75-d220-4458-ad42-45f8a2f87985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition 0: [0, 1, 2, 3, 4, 5]\nPartition 1: [6, 7, 8, 9, 10, 11]\nPartition 2: [12, 13, 14, 15, 16, 17]\nPartition 3: [18, 19, 20, 21, 22, 23, 24]\n"
     ]
    }
   ],
   "source": [
    "partitioned_values = rdd2.glom().collect()\n",
    "for i, partition in enumerate(partitioned_values):\n",
    "    print(f\"Partition {i}: {partition}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c97f4ca-875b-4333-9dab-8cead2728269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c8c3f1-28e9-4ac3-b5df-3dca580a23ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('James', 'Smith', 'USA', 'California'), ('Michael', 'Rose', 'USA', 'New York'), ('Robert', 'Williams', 'USA', 'California'), ('Maria', 'Jones', 'USA', 'Florida'), ('Pradeep', 'SV', 'India', 'India')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=spark.builder.appName('broadcast').getOrCreate()\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\",\"IN\":\"India\"}\n",
    "broadcast_states=spark.sparkContext.broadcast(states)\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\"),\n",
    "    (\"Pradeep\",\"SV\",\"India\",\"IN\")\n",
    "  ]\n",
    "columns=[\"Fname\",\"Lname\",\"country\",\"CD\"]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcast_states.value[code]\n",
    "result=rdd.map(lambda x:(x[0],x[1],x[2],state_convert(x[3]))).collect()\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0c1d27-cd2d-432c-88c2-293ed0200d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+----------+\n|  Fname|   Lname|country|        CD|\n+-------+--------+-------+----------+\n|  James|   Smith|    USA|California|\n|Michael|    Rose|    USA|  New York|\n| Robert|Williams|    USA|California|\n|  Maria|   Jones|    USA|   Florida|\n|Pradeep|      SV|  India|     India|\n+-------+--------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=spark.builder.appName('broadcast').getOrCreate()\n",
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"FL\":\"Florida\",\"IN\":\"India\"}\n",
    "broadcast_states=spark.sparkContext.broadcast(states)\n",
    "data = [(\"James\",\"Smith\",\"USA\",\"CA\"),\n",
    "    (\"Michael\",\"Rose\",\"USA\",\"NY\"),\n",
    "    (\"Robert\",\"Williams\",\"USA\",\"CA\"),\n",
    "    (\"Maria\",\"Jones\",\"USA\",\"FL\"),\n",
    "    (\"Pradeep\",\"SV\",\"India\",\"IN\")\n",
    "  ]\n",
    "columns=[\"Fname\",\"Lname\",\"country\",\"CD\"]\n",
    "df=spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "def state_convert(code):\n",
    "    return broadcast_states.value[code]\n",
    "result=df.rdd.map(lambda x:(x[0],x[1],x[2],state_convert(x[3]))).toDF(columns)\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "repartition and coalesce",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
